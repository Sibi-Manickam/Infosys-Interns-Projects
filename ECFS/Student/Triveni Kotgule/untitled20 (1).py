# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sZORMAE8MwAy5BYFKpDTQOeUs3fMFQXs
"""

import numpy as np
import pandas as pd
import math

import seaborn as sns
import xgboost as xgb
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import tensorflow as tf

energy = pd.read_csv('/content/energy_dataset.csv')
energy.head()
energy.describe().T
energy.rename(columns={
   'total load actual':'Energy Consumption'

}, inplace=True)

energy.tail(5)

colnames = ['generation fossil coal-derived gas', 'generation fossil oil shale','generation fossil peat',
           'generation geothermal', 'generation hydro pumped storage aggregated', 'generation marine',
           'generation wind offshore', 'forecast wind offshore eday ahead', 'forecast solar day ahead',
            'forecast wind onshore day ahead','price actual','total load forecast','price day ahead']
energy = energy.drop(colnames, axis = 1)

# def check_Nans_Dups(df_input):

#     print('Number of Nans in each column :')

#     print(df_input.isnull().sum())
#     print()/
1#     print(f'Number of duplicates in the dataframe : {df_input.duplicated().sum()}')
#     return
# check_Nans_Dups(df_energy)

energy.isnull().sum()

energy['time'] = pd.to_datetime(energy['time'])
energy = energy.set_index('time')
energy

energy.tail(5)

energy[energy.isna().any(axis = 1)]

energy.tail(5)

energy.interpolate(method='linear', limit_direction='forward', inplace = True)

energy.tail(5)

energy.isnull().sum()

energy.tail(5)

plt.figure(figsize=(12,6))

plt.plot(energy['Energy Consumption'][:24*7*2])
plt.xlabel('Time')
plt.ylabel('Energy Consumption')

plt.figure(figsize=(12,6))

plt.plot(energy['Energy Consumption'][:])
plt.xlabel('Time')
plt.ylabel('Energy Consumption')

def feat_corr(input_df):
    corr = input_df.corr()
    plt.figure(figsize=(15,12))

    g=sns.heatmap(corr,annot=True,cmap="BrBG", vmin=-1, vmax=1)
    plt.title('Feature Correlation')


    return plt.show()
feat_corr(energy)

energy.tail(5)

energy["generation fossil total"] = energy['generation fossil hard coal'] + energy['generation fossil brown coal/lignite']
energy.drop(['generation fossil hard coal', 'generation fossil brown coal/lignite'], axis = 1, inplace = True)

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Energy Consumption', y='generation biomass' , data=energy)

# Add labels and title
plt.xlabel('Energy Consumption')
plt.ylabel('generation biomass')
plt.title('Scatter Plot')

# Show the plot

plt.show()

sns.jointplot(x='Energy Consumption', y='generation biomass', data=energy)
plt.show()

energy.tail(5)

# Loading weather data
from sklearn.preprocessing import LabelEncoder

df_weather = pd.read_csv('/content/weather_features.csv')
df_temp = df_weather.copy(deep = True)
labels = ['weather_id', 'weather_main','weather_description','weather_icon']
for col in labels:
    df_temp[col] = LabelEncoder().fit_transform(df_weather[col])
df_weather.tail(5)

energy.tail(5)
df_weather.shape

df_temp = df_weather.copy(deep = True)
labels = ['weather_id', 'weather_main','weather_description','weather_icon']
for col in labels:
    df_temp[col] = LabelEncoder().fit_transform(df_weather[col])
df_weather.tail(5)

energy.tail(5)

col_drop_name = ['weather_id', 'weather_main','weather_description','weather_icon', 'temp_min', 'temp_max']
# col_drop_name = ['weather_id', 'weather_main','weather_description','weather_icon']
df_weather.drop(col_drop_name, axis = 1 , inplace = True)
df_weather.tail(5)

energy.tail(5)

# Checking Nans and duplicates in each columns
def check_Nans_Dups(df_input):

    print('Number of Nans in each column :')

    print(df_input.isnull().sum())
    print()
    print(f'Number of duplicates in the dataframe : {df_input.duplicated().sum()}')
    return
check_Nans_Dups(df_weather)
df_weather.tail(5)

df_weather = df_weather.reset_index().drop_duplicates()
df_weather.tail(5)

df_weather['time'] = pd.to_datetime(df_weather['dt_iso'])
df_weather.drop(["dt_iso"] , axis = 1, inplace = True)
df_weather = df_weather.set_index('time')

df_weather.tail(5)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))

columns_to_plot = ['pressure', 'wind_speed', 'rain_1h', 'rain_3h']

for i, ax in enumerate(axes.flat):
    if i < len(columns_to_plot):
        ax.plot(df_weather.index, df_weather[columns_to_plot[i]])
        ax.set_title(columns_to_plot[i])
    else:
        ax.set_visible(False)

plt.tight_layout()
plt.show()
df_weather.tail(5)
df_weather.drop(["index"] , axis = 1, inplace = True)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))

# select the columns to plot
columns_to_plot = ['pressure', 'wind_speed', 'rain_1h', 'rain_3h']

# loop through the subplots and plot each column
for i, ax in enumerate(axes.flat):
    if i < len(columns_to_plot):
        ax.boxplot(x = df_weather[columns_to_plot[i]])
        ax.set_title(columns_to_plot[i])
    else:
        ax.set_visible(False)

plt.tight_layout()  # adjust the spacing between subplots
plt.show()  # display the plotdf_weather.loc[df_weather['pressure']  > 1080,  'pressure'] = np.nan
df_weather.loc[df_weather['pressure']   < 870,  'pressure'] = np.nan
df_weather.loc[df_weather['wind_speed'] > 113, 'wind_speed'] = np.nan

df_weather.interpolate(method ='linear', limit_direction ='forward', inplace = True)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))

# select the columns to plot
columns_to_plot = ['pressure', 'wind_speed', 'rain_1h', 'rain_3h']

# loop through the subplots and plot each column
for i, ax in enumerate(axes.flat):
    if i < len(columns_to_plot):
        ax.boxplot(x = df_weather[columns_to_plot[i]])
        ax.set_title(columns_to_plot[i])
    else:
        ax.set_visible(False)

plt.tight_layout()  # adjust the spacing between subplots
plt.show()  # display the plot

df_weather.tail(5)

# be sure to drop rain_h3
df_weather.drop(['rain_3h'], axis = 1 , inplace = True)



print(f'Number of samples in df_energy is {energy.shape[0]}')

city_list = df_weather['city_name'].unique()
grouped_weather = df_weather.groupby('city_name')

for city in city_list:


    print(f'Number of samples in df_weather in {city} is {grouped_weather.get_group(city).shape[0]}')
df_weather.tail(5)

df_weather.tail(5)
df_weather.shape

df_weather_cleaned = df_weather.reset_index().drop_duplicates(subset=['time', 'city_name'], keep='first').set_index('time')

df_weather_cleaned.head()

print(f'Number of samples in df_energy is {energy.shape[0]}')

city_list = df_weather['city_name'].unique()
grouped_weather = df_weather_cleaned.groupby('city_name')

for city in city_list:
    print(f'Number of samples in df_weather in {city} is {grouped_weather.get_group(city).shape[0]}')

energy.shape

energy.tail(5)

# Reset the indices of energy and newweather
newweather = df_weather_cleaned.reset_index(drop=True)
# newweather.tail(5)
newweather.drop(columns=['city_name','clouds_all','snow_3h','rain_1h'], inplace=True)

newweather.tail(5)

# Reset the indices of energy and newweather
newweather = newweather.reset_index(drop=True)
energy_reset_index = energy.reset_index(drop=True)


# Merge the reset DataFrames
merged_data = pd.concat([energy_reset_index, newweather], axis=1)

merged_data.tail(5)

merged_data.dropna

cleaned_data = merged_data.dropna()

cleaned_data.tail(5)

feat_corr(cleaned_data)

# !pip install pmdarima

# from pmdarima import auto_arima
# stepwise_fit = auto_arima(cleaned_data['Energy Consumption'], trace=True,
# suppress_warnings=True)

# cleaned_data.shape

# train_data = cleaned_data.iloc[:-2000]  # All data except the last 20 entries
# test_data =  cleaned_data.iloc[-2000:]
# train_data.shape, test_data.shape

# from statsmodels.tsa.arima.model import ARIMA

# model = ARIMA(cleaned_data['Energy Consumption'], order=(1, 0, 5))
# model = model.fit()
# model.summary()

# start=len(train_data)
# end=len(train_data)+len(test_data)-1
# pred=model.predict(start=start,end=end,typ='levels').rename('ARIMA Predictions')
# pred.plot(legend=True)
# test_data['Energy Consumption'].plot(legend=True)

# from sklearn.metrics import mean_squared_error

# mse = mean_squared_error(test_data['Energy Consumption'], pred )


# rmse = np.sqrt(mse)
# print(f'RMSE: {rmse}')

# from sklearn.metrics import mean_squared_error
# import numpy as np

# mse = mean_squared_error(test_data['Energy Consumption'], pred)
# rmse = np.sqrt(mse)

# r2_score = 1 - (mse / np.var(test_data['Energy Consumption']))

# print(f'RMSE: {rmse}')
# print(f'R2 Score: {r2_score}')



# import pandas as pd
# import numpy as np
# from statsmodels.tsa.arima.model import ARIMA

# def predict_energy_arima(existing_model, cleaned_data, input_data, forecast_horizon=1):
#     """
#     Predict future energy consumption using an existing ARIMA model based on provided input data.

#     Parameters:
#     existing_model (ARIMAResults): Pre-fitted ARIMA model.
#     cleaned_data (DataFrame): Historical data.
#     input_data (dict): The last five days of energy generation data.
#     forecast_horizon (int): The number of future steps to forecast.

#     Returns:
#     DataFrame: Forecasted energy consumption.
#     """
#     features = [
#         'generation-biomass', 'generation-fossil-gas', 'generation-fossil-oil',
#         'generation-hydro-pumped-storage-consumption', 'generation-hydro-run-of-river-and-poundage',
#         'generation-hydro-water-reservoir', 'generation-nuclear', 'generation-other',
#         'generation-other-renewable', 'generation-solar', 'generation-waste'
#     ]

#     # Restructure input data into a DataFrame
#     days_data = []
#     for day in range(1, 6):
#         day_data = {feature: input_data.get(f'{feature}_day{day}', 0) for feature in features}
#         days_data.append(day_data)

#     input_df = pd.DataFrame(days_data)

#     # Sum each row to get total energy consumption per day
#     input_df['Energy Consumption'] = input_df.sum(axis=1)

#     # Predict using the pre-fitted ARIMA model
#     forecast = model.forecast(steps=forecast_horizon)

#     # Generate dates for the forecast
#     last_date = cleaned_data.index[-1]  # Assuming the last date in cleaned_data is the most recent
#     forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon)

#     # Create a DataFrame for the forecast
#     forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': forecast})
#     forecast_df.set_index('date', inplace=True)


#     return forecast_df

# # Define input data for the last five days
# input_data = {
#     'generation-biomass_day1': 100,
#     'generation-fossil-gas_day1': 200,
#     'generation-fossil-oil_day1': 150,
#     'generation-hydro-pumped-storage-consumption_day1': 120,
#     'generation-hydro-run-of-river-and-poundage_day1': 80,
#     'generation-hydro-water-reservoir_day1': 180,
#     'generation-nuclear_day1': 250,
#     'generation-other_day1': 90,
#     'generation-other-renewable_day1': 300,
#     'generation-solar_day1': 400,
#     'generation-waste_day1': 110,
#     'generation-biomass_day2': 110,
#     'generation-fossil-gas_day2': 210,
#     'generation-fossil-oil_day2': 160,
#     'generation-hydro-pumped-storage-consumption_day2': 130,
#     'generation-hydro-run-of-river-and-poundage_day2': 90,
#     'generation-hydro-water-reservoir_day2': 190,
#     'generation-nuclear_day2': 260,
#     'generation-other_day2': 100,
#     'generation-other-renewable_day2': 310,
#     'generation-solar_day2': 420,
#     'generation-waste_day2': 120,
#     'generation-biomass_day3': 120,
#     'generation-fossil-gas_day3': 220,
#     'generation-fossil-oil_day3': 170,
#     'generation-hydro-pumped-storage-consumption_day3': 140,
#     'generation-hydro-run-of-river-and-poundage_day3': 100,
#     'generation-hydro-water-reservoir_day3': 200,
#     'generation-nuclear_day3': 270,
#     'generation-other_day3': 110,
#     'generation-other-renewable_day3': 320,
#     'generation-solar_day3': 430,
#     'generation-waste_day3': 130,
#     'generation-biomass_day4': 130,
#     'generation-fossil-gas_day4': 230,
#     'generation-fossil-oil_day4': 180,
#     'generation-hydro-pumped-storage-consumption_day4': 150,
#     'generation-hydro-run-of-river-and-poundage_day4': 110,
#     'generation-hydro-water-reservoir_day4': 210,
#     'generation-nuclear_day4': 280,
#     'generation-other_day4': 120,
#     'generation-other-renewable_day4': 330,
#     'generation-solar_day4': 440,
#     'generation-waste_day4': 140,
#     'generation-biomass_day5': 140,
#     'generation-fossil-gas_day5': 240,
#     'generation-fossil-oil_day5': 190,
#     'generation-hydro-pumped-storage-consumption_day5': 160,
#     'generation-hydro-run-of-river-and-poundage_day5': 120,
#     'generation-hydro-water-reservoir_day5': 220,
#     'generation-nuclear_day5': 290,
#     'generation-other_day5': 130,
#     'generation-other-renewable_day5': 340,
#     'generation-solar_day5': 450,
#     'generation-waste_day5': 150
# }

# # Assume cleaned_data is a DataFrame with historical energy consumption data
# date_rng = pd.date_range(start='2023-01-01', end='2023-02-28', freq='D')
# cleaned_data = pd.DataFrame(date_rng, columns=['date'])
# cleaned_data['Energy Consumption'] = np.random.randint(1000, 5000, size=(len(date_rng)))
# cleaned_data.set_index('date', inplace=True)

# # Load the pre-fitted ARIMA model
# # Replace `arima_model_fit` with your actual fitted model variable
# arima_model_fit = ARIMA(cleaned_data['Energy Consumption'], order=(1, 0, 5)).fit()

# # Call the predict_energy_arima function with the pre-fitted ARIMA model and input data
# predictions_df = predict_energy_arima(arima_model_fit, cleaned_data, input_data)

# # Print the predictions
# print("Predictions:\n", predictions_df)



# import pandas as pd
# from statsmodels.tsa.arima.model import ARIMA
# import warnings

# # Ignore warnings for cleaner output
# warnings.filterwarnings('ignore')

# def predict_energy_arima(existing_model, cleaned_data, input_dates, input_data):
#     """
#     Predict future energy consumption using an existing ARIMA model based on provided input data.

#     Parameters:
#     existing_model (ARIMAResults): Pre-fitted ARIMA model.
#     cleaned_data (DataFrame): Historical data with 'Energy Consumption'.
#     input_dates (list): List of dates for which predictions are needed.
#     input_data (dict): The energy generation data for these dates.

#     Returns:
#     float: The predicted energy consumption value for the last input date.
#     """
#     # Check if input dates are within the existing dataset
#     last_date_in_data = cleaned_data.index[-1]
#     input_date_range = pd.to_datetime(input_dates)

#     if input_date_range.max() <= last_date_in_data:
#         # Input dates are within the existing dataset; use existing model for prediction
#         predictions = existing_model.predict(start=input_date_range[0], end=input_date_range[-1])
#         last_predicted_value = predictions[-1]
#     else:
#         # Extract features for the new input data
#         features = [
#             'generation-biomass', 'generation-fossil-gas', 'generation-fossil-oil',
#             'generation-hydro-pumped-storage-consumption', 'generation-hydro-run-of-river-and-poundage',
#             'generation-hydro-water-reservoir', 'generation-nuclear', 'generation-other',
#             'generation-other-renewable', 'generation-solar', 'generation-waste'
#         ]

#         # Create a DataFrame for input data
#         input_df = pd.DataFrame(index=pd.to_datetime(input_dates))
#         for feature in features:
#             input_df[feature] = [input_data.get(f'{feature}_day{idx+1}', 0) for idx in range(len(input_dates))]

#         # Calculate energy consumption for each input date
#         input_df['Energy Consumption'] = input_df.sum(axis=1)

#         # Extend the original data with the input data
#         extended_data = pd.concat([cleaned_data, input_df])

#         # Determine the forecast horizon
#         forecast_horizon = (input_date_range[-1] - last_date_in_data).days
#         if forecast_horizon <= 0:
#             raise ValueError("The provided input dates must be after the last date in the original dataset.")

#         # Refit the ARIMA model on the extended data
#         extended_model = ARIMA(extended_data['Energy Consumption'], order=(1, 0, 1)).fit()

#         # Forecast future energy consumption
#         forecast = extended_model.forecast(steps=forecast_horizon)
#         last_predicted_value = forecast[-1]

#     return last_predicted_value

# # Example user input for dates and energy generation data
# input_dates = [
#     '2023-01-08', '2023-01-09', '2023-01-10', '2023-01-11', '2023-01-12'
# ]
# input_data = {
#     'generation-biomass_day1': 100,
#     'generation-fossil-gas_day1': 200,
#     'generation-fossil-oil_day1': 150,
#     'generation-hydro-pumped-storage-consumption_day1': 120,
#     'generation-hydro-run-of-river-and-poundage_day1': 80,
#     'generation-hydro-water-reservoir_day1': 180,
#     'generation-nuclear_day1': 250,
#     'generation-other_day1': 90,
#     'generation-other-renewable_day1': 300,
#     'generation-solar_day1': 400,
#     'generation-waste_day1': 110,
#     'generation-biomass_day2': 110,
#     'generation-fossil-gas_day2': 210,
#     'generation-fossil-oil_day2': 160,
#     'generation-hydro-pumped-storage-consumption_day2': 130,
#     'generation-hydro-run-of-river-and-poundage_day2': 90,
#     'generation-hydro-water-reservoir_day2': 190,
#     'generation-nuclear_day2': 260,
#     'generation-other_day2': 100,
#     'generation-other-renewable_day2': 310,
#     'generation-solar_day2': 420,
#     'generation-waste_day2': 120,
#     'generation-biomass_day3': 120,
#     'generation-fossil-gas_day3': 220,
#     'generation-fossil-oil_day3': 170,
#     'generation-hydro-pumped-storage-consumption_day3': 140,
#     'generation-hydro-run-of-river-and-poundage_day3': 100,
#     'generation-hydro-water-reservoir_day3': 200,
#     'generation-nuclear_day3': 270,
#     'generation-other_day3': 110,
#     'generation-other-renewable_day3': 320,
#     'generation-solar_day3': 430,
#     'generation-waste_day3': 130,
#     'generation-biomass_day4': 130,
#     'generation-fossil-gas_day4': 230,
#     'generation-fossil-oil_day4': 180,
#     'generation-hydro-pumped-storage-consumption_day4': 150,
#     'generation-hydro-run-of-river-and-poundage_day4': 110,
#     'generation-hydro-water-reservoir_day4': 210,
#     'generation-nuclear_day4': 280,
#     'generation-other_day4': 120,
#     'generation-other-renewable_day4': 330,
#     'generation-solar_day4': 440,
#     'generation-waste_day4': 140,
#     'generation-biomass_day5': 140,
#     'generation-fossil-gas_day5': 240,
#     'generation-fossil-oil_day5': 190,
#     'generation-hydro-pumped-storage-consumption_day5': 160,
#     'generation-hydro-run-of-river-and-poundage_day5': 120,
#     'generation-hydro-water-reservoir_day5': 220,
#     'generation-nuclear_day5': 290,
#     'generation-other_day5': 130,
#     'generation-other-renewable_day5': 340,
#     'generation-solar_day5': 450,
#     'generation-waste_day5': 150
# }

# # Example historical data provided by the user
# historical_data = {
#     'dates': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06', '2023-01-07'],
#     'energy_consumption': [1000, 1500, 1300, 1600, 1800, 1700, 1400]
# }

# # Convert historical data into a DataFrame
# cleaned_data = pd.DataFrame({
#     'date': pd.to_datetime(historical_data['dates']),
#     'Energy Consumption': historical_data['energy_consumption']
# }).set_index('date')

# # Load and fit the ARIMA model on historical data
# arima_model_fit = ARIMA(cleaned_data['Energy Consumption'], order=(1, 0, 1)).fit()

# # Predict energy consumption using the updated predict_energy_arima function
# last_prediction = predict_energy_arima(arima_model_fit, cleaned_data, input_dates, input_data)

# # Print the last predicted value
# print("Last Predicted Energy Consumption:", last_prediction)

# !pip install --upgrade gradio

# import pandas as pd
# import numpy as np
# import gradio as gr
# import matplotlib.pyplot as plt
# from statsmodels.tsa.arima.model import ARIMA

# # Load your ARIMA model (replace with your actual model)
# # arima_model_fit = ARIMA(cleaned_data['Energy Consumption'], order=(1, 0, 5)).fit()

# # Define the features used by the model
# features = [
#     'generation-biomass', 'generation-fossil-gas', 'generation-fossil-oil',
#     'generation-hydro-pumped-storage-consumption', 'generation-hydro-run-of-river-and-poundage',
#     'generation-hydro-water-reservoir', 'generation-nuclear', 'generation-other',
#     'generation-other-renewable', 'generation-solar', 'generation-waste'
# ]

# def create_lagged_features(df, lags):
#     """
#     Create lagged features for a DataFrame.

#     Parameters:
#     df (DataFrame): The input DataFrame with original features.
#     lags (int): The number of lagged time steps to include.

#     Returns:
#     DataFrame: DataFrame with lagged features.
#     """
#     lagged_df = pd.DataFrame()
#     for col in df.columns:
#         for lag in range(1, lags + 1):
#             lagged_df[f'{col}_lag{lag}'] = df[col].shift(lag)

#     # Drop rows with NaN values caused by the lagging
#     lagged_df.dropna(inplace=True)

#     return lagged_df

# def predict_energy_arima(existing_model, cleaned_data, input_data, forecast_horizon=10):
#     """
#     Predict future energy consumption using an existing ARIMA model based on provided input data.

#     Parameters:
#     existing_model (ARIMAResults): Pre-fitted ARIMA model.
#     cleaned_data (DataFrame): Historical data.
#     input_data (dict): The last five days of energy generation data.
#     forecast_horizon (int): The number of future steps to forecast.

#     Returns:
#     DataFrame: Forecasted energy consumption.
#     """
#     # List of features used by the model
#     features = [
#         'generation-biomass', 'generation-fossil-gas', 'generation-fossil-oil',
#         'generation-hydro-pumped-storage-consumption', 'generation-hydro-run-of-river-and-poundage',
#         'generation-hydro-water-reservoir', 'generation-nuclear', 'generation-other',
#         'generation-other-renewable', 'generation-solar', 'generation-waste'
#     ]

#     # Restructure input data into a DataFrame
#     days_data = []
#     for day in range(1, 6):
#         day_data = {feature: input_data.get(f'{feature}_day{day}', 0) for feature in features}
#         days_data.append(day_data)

#     input_df = pd.DataFrame(days_data)

#     # Sum each row to get total energy consumption per day
#     input_df['Energy Consumption'] = input_df.sum(axis=1)

#     # Predict using the pre-fitted ARIMA model
#     forecast = existing_model.forecast(steps=forecast_horizon)

#     # Generate dates for the forecast
#     last_date = cleaned_data.index[-1]  # Assuming the last date in cleaned_data is the most recent
#     forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon)

#     # Create a DataFrame for the forecast
#     forecast_df = pd.DataFrame({'date': forecast_dates, 'forecast': forecast})
#     forecast_df.set_index('date', inplace=True)

#     return forecast_df

# def display_forecast_plot(forecast_df):
#     """
#     Display a line plot showing historical data and forecasted energy consumption.

#     Parameters:
#     forecast_df (DataFrame): Forecasted energy consumption data.

#     Returns:
#     plt.Figure: Matplotlib figure object.
#     """
#     # Plot historical data
#     plt.plot(cleaned_data.index, cleaned_data['Energy Consumption'], label='Historical Data', color='blue')

#     # Plot forecasted data
#     plt.plot(forecast_df.index, forecast_df['forecast'], label='Forecast', color='red')

#     plt.xlabel('Date')
#     plt.ylabel('Energy Consumption')
#     plt.title('Energy Consumption Forecast')
#     plt.legend()
#     plt.grid(True)

#     return plt.gcf()

# # Define manual input components for Gradio
# manual_inputs = []
# for day in range(1, 6):
#     for feature in features:
#         manual_inputs.append(gr.Slider(
#             minimum=0, maximum=1000, step=0.1,
#             label=f'{feature.replace("_", " ").capitalize()} (Day {day})'
#         ))

# # Define the Gradio interface
# interface = gr.Interface(
#     fn=predict_energy_arima,
#     inputs=[gr.File(label="Upload CSV File (Optional)")] + manual_inputs,
#     outputs=[gr.Image(type="numpy", label="Energy Consumption Forecast")],
#     title="Energy Consumption Forecast using ARIMA",
#     description="Predict future energy consumption based on the last five days of energy generation data using ARIMA model.",
# )

# # Launch the interface
# interface.launch()

# pip install git+https://github.com/gradio-app/gradio.git



# pip install gradio

# # Define manual input components for Gradio
# manual_inputs = []
# for day in range(1, 6):
#     for feature in features:
#         manual_inputs.append(gr.Slider(
#             minimum=0, maximum=1000, step=0.1,
#             label=f'{feature.replace("_", " ").capitalize()} (Day {day})'
#         ))

# !pip install --upgrade gradio

# pip install -U gradio

import pandas as pd

# Assuming df_weather_energy is already defined and contains 'Energy Consumption' column
testtraindata = pd.DataFrame()  # Ensure testtraindata is an empty DataFrame

# Add 'energy consumption' column to testtraindata
testtraindata['energy consumption'] = cleaned_data['Energy Consumption'].values

# Define the number of lag periods
num_lags = 7

# Add lagged columns to testtraindata
for lag in range(1, num_lags + 1):
    testtraindata[f'lag_{lag}'] = testtraindata['energy consumption'].shift(lag)

# Display the first few rows to check if the lagged columns are populated
print(testtraindata.head(30))
  # Check the column names

testtraindata.values

testtraindata.head(30)



# import pandas as pd
# from statsmodels.tsa.arima.model import ARIMA
# from sklearn.metrics import mean_absolute_error
# import matplotlib.pyplot as plt

# # Split data into train and test sets
# train_size = int(len(testtraindata) * 0.8)
# train, test = testtraindata[:train_size], testtraindata[train_size:]

# merged_data.shape
# merged_data.reset_index(drop=True, inplace=True)

# merged_data.dropna()

# import pandas as pd
# from statsmodels.tsa.arima.model import ARIMA
# from sklearn.metrics import mean_absolute_error
# import matplotlib.pyplot as plt

# # Split data into train and test sets
# train_size = int(len(merged_data) * 0.8)
# train, test = merged_data[:train_size], merged_data[train_size:]

train

testtraindata

data_without_index = testtraindata.values
testtraindata_no_index = pd.DataFrame(data_without_index, columns=testtraindata.columns)
testtraindata_no_index
testtraindata_no_index.reset_index(drop=True, inplace=True)
testtraindata= testtraindata_no_index





# X_train=train.drop(['Energy Consumption'],axis=1)
# y_train=train['Energy Consumption']
# X_test=test.drop(['Energy Consumption'],axis=1)
# y_test=test['Energy Consumption']

# X_train

X_test

features

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
# Assuming testtraindata contains the lagged columns and the target 'energy consumption'
# Drop rows with NaN values generated by shifting
testtraindata.dropna(inplace=True)

# Define features and target
features = testtraindata.drop(['energy consumption'], axis=1)
target = testtraindata['energy consumption']

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_features = scaler.fit_transform(features)
scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(scaled_features, scaled_target, test_size=0.2, shuffle=False)

# Reshape input to be 3D (samples, timesteps, features) as required by LSTM
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))



X_train

y_test

y_train



# Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, batch_size=1, epochs=10, validation_data=(X_test, y_test))

X_test

y_train

test_predictions

final

Inverted user input DataFrame:
                      0      1      2      3      4      5      6
total_generation  20341  22532  27117  24792  25870  29809  29670
array([[20341, 22532, 27117, 24792, 25870, 29809, 29670]])

total_generation_array[0]

final=model.predict(total_generation_array)
final = scaler.inverse_transform(final)

# Make predictions
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)

# Inverse transform predictions and actual values
train_predictions = scaler.inverse_transform(train_predictions)
y_train = scaler.inverse_transform(y_train)
test_predictions = scaler.inverse_transform(test_predictions)
y_test = scaler.inverse_transform(y_test)
final=model.predict(total_generation_array)
final = scaler.inverse_transform(final)

# Make predictions
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)

# Inverse transform predictions and actual values
train_predictions = scaler.inverse_transform(train_predictions)
y_train = scaler.inverse_transform(y_train)
test_predictions = scaler.inverse_transform(test_predictions)
y_test = scaler.inverse_transform(y_test)


# Plot the results
plt.figure(figsize=(14, 5))
plt.plot(y_train, label='Train Actual')
plt.plot(train_predictions, label='Train Predicted')
plt.title('Train Data: Actual vs Predicted')
plt.legend()
plt.show()

plt.figure(figsize=(14, 5))
plt.plot(y_test, label='Test Actual')
plt.plot(test_predictions, label='Test Predicted')
plt.title('Test Data: Actual vs Predicted')
plt.legend()
plt.show()

y_test

train_predictions = model.predict(X_train)
train_predictions

train_predictions=scaler.inverse_transform(train_predictions)
train_predictions

X_test



xx

from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, test_predictions )


rmse = np.sqrt(mse)
print(f'RMSE: {rmse}')

from sklearn.metrics import r2_score
r2 = r2_score(y_test,test_predictions)

# Print the R² score
print(f"R² Score: {r2}")

import pandas as pd
import numpy as np

def generate_random_input():
    """
    Function to generate random input values for the specified features,
    sum them up, and return the DataFrame with the summed values.
    """
    # Generate random values for the input features
    random_values = {
        'generation-fossil-gas_day1': np.random.randint(1500, 1600, 7),
        'generation-fossil-oil_day1': np.random.randint(120, 11100, 7),
        'generation-hydro-pumped-storage-consumption_day1': np.random.randint(100, 1150, 7),
        'generation-hydro-run-of-river-and-poundage_day1': np.random.randint(1810, 11250, 7),
        'generation-hydro-water-reservoir_day1': np.random.randint(1100, 1300, 7),
        'generation-nuclear_day1': np.random.randint(1150, 1300, 7),
        'generation-other_day1': np.random.randint(110, 150, 7),
        'generation-other-renewable_day1': np.random.randint(1120, 11300, 7),
        'generation-solar_day1': np.random.randint(1200, 1400, 7),
        'generation-waste_day1': np.random.randint(10, 50, 7)
    }

    # Create a DataFrame from the random values
    random_input_df = pd.DataFrame(random_values)

    # Calculate total generation for each day
    random_input_df['total_generation'] = random_input_df.sum(axis=1)

    return random_input_df

# Example usage of the function for generating random input
print("Random input DataFrame:")
random_input_df = generate_random_input()

total_generation = random_input_df["total_generation"]
total_generation_df = pd.DataFrame(total_generation)

import pandas as pd

def get_user_input():
    """
    Function to get user input values for the specified features,
    sum them up, and return the DataFrame with the summed values.
    """
    features = [
        'generation-fossil-gas_day1',
        'generation-fossil-oil_day1',
        'generation-hydro-pumped-storage-consumption_day1',
        'generation-hydro-run-of-river-and-poundage_day1',
        'generation-hydro-water-reservoir_day1',
        'generation-nuclear_day1',
        'generation-other_day1',
        'generation-other-renewable_day1',
        'generation-solar_day1',
        'generation-waste_day1'
    ]

    # Dictionary to store user inputs
    user_values = {feature: [] for feature in features}

    # Number of days to input data
    num_days = 7

    # Get user input for each day and feature
    for day in range(1, num_days + 1):
        print(f"\nEnter data for Day {day}:")
        for feature in features:
            while True:
                try:
                    value = float(input(f"Enter value for {feature} (Day {day}): "))
                    user_values[feature].append(value)
                    break
                except ValueError:
                    print("Invalid input. Please enter a numerical value.")

    # Create a DataFrame from the user values
    user_input_df = pd.DataFrame(user_values)

    # Calculate total generation for each day
    user_input_df['total_generation'] = user_input_df.sum(axis=1)

    return user_input_df

# Example usage of the function for getting user input
print("Please enter your data manually:")
user_input_df = get_user_input()

# Print the DataFrame with the user input data
print("\nUser input DataFrame:")
print(user_input_df)

# Extracting the total generation and converting it to DataFrame
total_generation = user_input_df["total_generation"]
total_generation_df = pd.DataFrame(total_generation)
print("\nTotal Generation DataFrame:")
print(total_generation_df)

# Transpose the DataFrame user_input_df
total_generation_df= total_generation_df.T

print("Inverted user input DataFrame:")
print(total_generation_df)
total_generation_df.values



# scaler = MinMaxScaler(feature_range=(0, 1))
# scaled_features = scaler.fit_transform(inverted_user_input_df)

# scaler = MinMaxScaler(feature_range=(0, 1))
# scaled_features = scaler.fit_transform(features)
# scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))

# # Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(scaled_features, scaled_target, test_size=0.2, shuffle=False)

# # Reshape input to be 3D (samples, timesteps, features) as required by LSTM
# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

total_generation_array = total_generation_df.values.reshape(-1, 1)

# Fit and transform the data
total_generation_array= scaler.fit_transform(total_generation_array)

total_generation_array= scaler.fit_transform(total_generation_array)

total_generation_array

total_generation_array=total_generation_array.reshape((total_generation_array.shape[0],total_generation_array.shape[1],1))

total_generation_array.shape

total_generation_array



total_generation_array

final=model.predict(total_generation_array)

final

final = scaler.inverse_transform(final)

final

xx=X_test[0:1]
xx
test_predictions = model.predict(xx)
xx
test_predictions = scaler.inverse_transform(test_predictions)
test_predictions

final[0]

# from sklearn.preprocessing import MinMaxScaler

# # Initialize the MinMaxScaler
# scaler = MinMaxScaler()

# # Fit and transform the data
# normalized_values = scaler.fit_transform(total_generation_df)

# # Create a DataFrame from the normalized values
# normalized_df = pd.DataFrame(normalized_values, columns=total_generation_df.columns, index=total_generation_df.index)

# print("Normalized DataFrame:")
# print(normalized_df)


# this is used if you have the dataset with you



from sklearn.preprocessing import MinMaxScaler

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Reshape the array to fit the scaler's requirements
total_generation_array = total_generation.values.reshape(-1, 1)

# Fit and transform the data
scaled_total_generation = scaler.fit_transform(total_generation_array)

# Print the normalized array
print(scaled_total_generation)

print(scaled_total_generation)

# Assuming 'model' is your trained LSTM model and 'scaled_total_generation' is your input data

# 1. Make predictions with the LSTM model
test_predictions_scaled = model.predict(scaled_total_generation)

# 2. Inverse transform the predictions


# Ensure 'y_test' is correctly formatted and adjust its length if necessary
# Then, inverse transform 'y_test' if it was scaled
# y_test = scaler.inverse_transform(y_test)

print(merged_data.index)

# Assuming you have your LSTM model named 'model'
# Assuming your new data is named 'new_data'
import pandas as pd

# Use the predict method of the LSTM model
predictions = model.predict(scaled_total_generation)

# Convert the prediction array to a DataFrame or Series with the appropriate index
prediction_index = scaled_total_generation.index  # Use the index of your new data
predictions = pd.Series(predictions, index=prediction_index)

# Print or use the predictions as needed
print(predictions)

# Reshape the array to fit the LSTM model's requirements
scaled_total_generation_reshaped = scaled_total_generation.values.reshape((1, scaled_total_generation.shape[0], 1))

# Use the predict method of the LSTM model
predictions = model.predict(scaled_total_generation_reshaped)

# Convert the prediction array to a DataFrame or Series with the appropriate index
prediction_index = scaled_total_generation.index  # Use the index of your new data
predictions = pd.Series(predictions.flatten(), index=prediction_index)

# Print or use the predictions as needed
print(predictions)

scaled_total_generation

import pandas as pd

# Assuming your new data is stored in a DataFrame named 'new_data'
# Create a new index for the DataFrame
index = range(len(scaled_total_generation))

# Assign the index to the DataFrame
scaled_total_generation.index = index

# Now your DataFrame has an index starting from 0 to len(new_data) - 1

scaled_total_generation



print(scaled_total_generation)
scaled_total_generation

